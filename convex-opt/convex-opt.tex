\documentclass[12pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{pgfplots}
\usepackage{mathtools}
\usepackage{booktabs}
\usepackage{indentfirst}
\usepackage{hyperref}

\usetikzlibrary{angles, quotes}

\pgfplotsset{compat=newest}

\title{Convex Optimization}
\author{Linxuan Ma}

\newcommand{\mo}[1]{\lvert #1 \rvert}
\newcommand{\mos}[1]{\lvert #1 \rvert^2}
\newcommand{\mov}[1]{\lvert \vec{#1} \rvert}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\p}{\partial}
\newcommand{\iv}[1]{\langle #1 \rangle}
\newcommand{\adj}{\text{adj}}
\newcommand{\dom}{\text{dom}}
\newcommand{\st}{\text{s.t. }}

\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]
\newtheorem{ex}{Exercise}

\begin{document}
	\maketitle
	
	\abstract{Convex optimization is a subset of mathematical optimizations that centers around optimization of convex functions over convex sets. The following note is taken from CMU 10-725 (watched from YouTube) as well as the book \emph{Convex Optimization} (Boyd. et al)}. My attempt at relevant programs and exercises can be found at \url{https://github.com/davidmaamoaix/convex-opt}.
	
	\section{Introduction}
	
	Optimization is the act of minimizing (or maximizing) a function while conforming to certain constraints.
	
	\subsection{Examples of Optimization Problems}
	
	Examples of \emph{regressions}:
	\begin{itemize}
		\item least squares: \begin{gather*}
			\min_\beta \sum_{i=1}^n(y_i - x_i^\top \beta)^2
		\end{gather*}
		\item least absolute deviations (robust): \begin{gather*}
			\min_\beta \sum_{i=1}^n \mo{y_i - x_i^\top \beta}
		\end{gather*}
		\item lasso (with constraints): \begin{gather*}
			\min_\beta \sum_{i=1}^n(y_i - x_i^\top \beta)^2 \\\text{s.t.} \sum_j^m |\beta_j| \le t
		\end{gather*}
		\end{itemize}
		
	Examples of \emph{classifications}:
	\begin{itemize}
		\item logistic regressions
		\item range Loss (SVMs)
	\end{itemize}
	
	Others:
	\begin{itemize}
		\item traveling salesman
		\item planning/discrete optimizations
		\item MLE
	\end{itemize}
	
	The following problems are \textbf{not} optimization problems, as their output isn't simple optimal in respect to some criteria:
	\begin{itemize}
		\item boosting
		\item ensembles (e.g. random forest)
		\item CV
	\end{itemize}
	
	\subsubsection{Example: 2D Fused Lasso}
	2D fused lasso, or 2D total variation de-noising, fits a piecewise constant function over an image ($\lambda$ is a hyper-parameter to determine the penalty, and $E$ is the set of edges from a pixel $i$ to all adjacent pixels $j$):
	\begin{gather*}
		\min_\theta \frac{1}{2} \sum_{i=1}^n (y_i - \theta_i) ^ 2  + \lambda \sum_{(i, j) \in E} |\theta_i - \theta_j|
	\end{gather*}
	
	Some methods to approach the 2D fused lasso problem (details on algorithms later):
	\begin{itemize}
		\item Specialized Alternating Direction Method of Multipliers (ADMM): fast (structured subproblem)
		\item Proximal Gradient: slow (poor conditioning)
		\item Coordinate Descent: slow (large active set)
	\end{itemize}
	
	\textbf{Conclusion}: different algorithms work better in different optimization problems.
	
	\subsubsection{Example: 1D Fused Lasso}
	
	1D fused lasso is similar to its 2D equvalent:
	\begin{gather*}
		\min_\theta \frac{1}{2} \sum_{i=1}^n (y_i - \theta_i) ^ 2  + \lambda \sum_{i=1}^{n - 1} |\theta_i - \theta_{i + 1}|
	\end{gather*}
	
	Trivially, as $\lambda$ decreases, more change points appear. We tune $\lambda$ to fit the more significant change points.
	
	\subsection{Convexity}
	
	Convexity is an attribute of a function or set.
	
	\begin{defn}
		$C$ is a \emph{convex set} if:
		\begin{gather*}
			\forall x, y \in C\ldotp t \in [0, 1] \Rightarrow tx + (1-t) y \in C
		\end{gather*}
	\end{defn}
	
	An example of a convex set is the set of points in a convex shape.
	
	\begin{defn}
		A \emph{convex function} is a function that:
		\begin{enumerate}
			\item has a convex domain:
			\begin{gather*}
				f: \RR^n \to \RR \Rightarrow \dom(f) \in \RR^n\ \text{is convex}
			\end{gather*}
			\item any value on the function is less than or equal to the linear equation joined by any two surrounding points:
			\begin{gather*}
				f(tx + (1 - t)y) \le tf(x) + (1 - t)f(y)
			\end{gather*}
		\end{enumerate}
	\end{defn}
	
	\subsection{Optimization Problem}
	
	\begin{defn}
		Any optimization problem can be rewritten as the following:
		\begin{align*}
			&\min_{x \in D} f(x) \\
			\st &g_i(x) \leq 0,\ i = 0, \dots, m \\
			& h_j(x) = 0,\ j = 0, \dots, r
		\end{align*}
		where $D$ is the common domain of the three functions:
		\begin{gather*}
			D = \dom(f) \cap \bigcap_{i=1}^m dom(g_i) \cap \bigcap_{j=1}^r dom(h_j)
		\end{gather*}
	\end{defn}
	
	A convex problem is an optimization problem such that $f$ (the criterion) and all $g$ (constraints) are convex functions, and all $h$ are affine functions.
	
\end{document}
