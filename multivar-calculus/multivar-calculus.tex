\documentclass[12pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{pgfplots}
\usepackage{mathtools}
\usepackage{booktabs}
\usepackage{indentfirst}

\usetikzlibrary{angles, quotes}

\pgfplotsset{compat=newest}

\title{Multivariable Calculus}
\author{Linxuan Ma}

\newcommand{\mo}[1]{\lvert #1 \rvert}
\newcommand{\mos}[1]{\lvert #1 \rvert^2}
\newcommand{\mov}[1]{\lvert \vec{#1} \rvert}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\p}{\partial}
\newcommand{\iv}[1]{\langle #1 \rangle}
\newcommand{\adj}{\text{adj}}
\newcommand*\diff{\mathop{}\!\mathrm{d}}

\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]
\newtheorem{ex}{Exercise}

\begin{document}
	\maketitle
	
	\abstract{Multivariable calculus is a useful concept in mathematical analysis. This document focuses on the differential and integral for multivariable calculus. The following notes are taken from MIT OCW $18.02$ Multivariable Calculus.}
	
	\section{Vectors}
	
	A vector $\vec{v}$ is a mathematical structure that has represents a tuple of direction and magnitude; its elements correspond to the cartesian coordinates of the represented point. A vector $\vec{v} \in \RR^n$ represents a point in n-dimension.
	
	The length $\mov{v}$ of $\vec{v} \in \RR^n$ is a scalar that represents its magnitude: $$\mov{v} = \sqrt{\sum_{i=0}^{n}(\vec{v}_i)^2}$$
	
	Note that a vector does not necessarily have a starting and ending point;  it merely represents an offset in a direction.
	
	\subsection{Vector Arithmetic}
	
	Multiplying a vector by a scalar scales the individual elements by the scalar:
	\begin{equation*}
		\begin{bmatrix}
			a \\ b
		\end{bmatrix} * c = 
		\begin{bmatrix}
			ac \\ bc
		\end{bmatrix}
	\end{equation*}
	
	Addition of vectors is element-wise:
	\begin{equation*}
	\begin{bmatrix}
		a \\ b
	\end{bmatrix} +
	\begin{bmatrix}
		c \\ d
	\end{bmatrix} =
	\begin{bmatrix}
		a + c \\ b + d
	\end{bmatrix}
	\end{equation*}
	
	\subsection{Dot Product}
	
	\begin{defn}
		The dot product of $\vec{a}$ and $\vec{b}$ is the sum of their element-wise product: $$\vec{a} * \vec{b} = \sum_{i=0}^n \vec{a}_i \vec{b}_i$$
	\end{defn}
	
	Geometrically, the product of two vector is the product of their magnitude and the $\cos$ of their angle: $$\vec{a} * \vec{b} = \mo{\vec{a}} \mo{\vec{b}} \cos \theta$$
	
	Consider vector $\vec{a} * \vec{a}$, the resulting product should be   $\mov{a}^2$, as $\cos 0$ is $1$. Considering a triangle with angles $a$, $b$ and $c$ with angle $\theta$ opposing side $c$, the above can be deduced from the law of $\cos$: $$\mov{c}^2 = \mov{a}^2 + \mov{b}^2 - 2\mo{a}\mo{b}\cos \theta$$
	
	Since $\vec{c} = \vec{a} - \vec{b}$, it can be deduced that:
	\begin{align*}
		\mov{c}^2 = \vec{c} * \vec{c} &= (\vec{a} - \vec{b})*(\vec{a} - \vec{b}) \\
		&= \vec{a} * \vec{a} - \vec{a} * \vec{b} - \vec{b} * \vec{a} + \vec{b} * \vec{b}\\
		&= \mov{a}^2 + \mov{b}^2 - 2\vec{a}\vec{b}
	\end{align*}
	
	As shown in the last line of the transformation of $\mov{c}^2$, $-2\vec{a}\vec{b} = -2\mov{a}\mov{b} \cos \theta$.
	
	\subsection{Applications of Dot Product}
	
	Dot product can be used to compute lengths and angles. Consider a $\RR^3$ space with $P = (1, 0, 0)$, $Q = (0, 1, 0)$ and $R = (0, 0, 2)$, the angle $\angle RPQ$ can be found with dot product:
	\begin{align*}
		\overrightarrow{PR} * \overrightarrow{PQ} &= \mo{\overrightarrow{PR}} \mo{\overrightarrow{PQ}} \cos \theta \\
		\cos \theta &= \frac{\overrightarrow{PQ} * \overrightarrow{PR}}{\mo{\overrightarrow{PQ}}\mo{\overrightarrow{PR}}}
	\end{align*}
	
	By plugging in our example, we obtain:
	\begin{gather*}
		\cos \theta = \frac{1 + 0 + 0}{\sqrt{2} * \sqrt{5}} = \frac{1}{\sqrt{10}}
	\end{gather*}
	
	Another application of dot product is to determine the orthogonality of two vectors, i.e. when are two vectors $a$ and $b$ perpendicular. Note that the sign of a dot product denotes the directional relation of the two vectors $a$ and $b$ with angle $\theta$:
	\begin{itemize}
		\item $\vec{a} * \vec{b} > 0$ if $\theta < 90$
		\item $\vec{a} * \vec{b} = 0$ if $\theta = 90$
		\item $\vec{a} * \vec{b} > 0$ if $\theta > 90$
	\end{itemize}
	
	Consider the linear equation $x + 2y + 3z = 0$. The above equation can be written in the form of a dot product:
	\begin{gather*}
		\begin{bmatrix}
			1 \\ 2 \\ 3
		\end{bmatrix} *
		\begin{bmatrix}
			x \\ y \\ z
		\end{bmatrix} = 0
	\end{gather*}
	Therefore, the vector $\iv{x, y, z}$ is perpendicular to $\iv{1, 2, 3}$, making the former a plane, as $\cos \theta = 0$ where $\theta$ is the angle between the two vectors.
	
	\section{Cross Product}
	
	To get the component $a_u$ of $\vec{a}$ along unit vector $\vec{u}$:
	\begin{align*}
		a_u &= \mov{a}\cos{\theta} \\
		&= \mov{a}\mov{u}\cos{\theta} \\
		&= \vec{a} * \vec{u}
	\end{align*}
	
	\subsection{Geometric Interpretation of Determinants}
	
	To get the area of triangle $A$ with two edges denoted as vector $\vec{a}$ and $\vec{b}$ originating from point $O$, the area of $A$ is:
	\begin{gather*}
		\text{Area}(A) = \frac{1}{2} \mov{a}\mov{b} \sin \theta
	\end{gather*}
	
	The above could be obtained with dot product considering the complement angle and $\vec{a'}$ as $\vec{a}$ after $90^\circ$ rotation. $\angle a'Oa = \theta'$:
	\begin{align*}
		\text{Area}(A) &= \frac{1}{2} \mov{a}\mov{b} \sin \theta \\
		&= \vec{a'} * \vec{b} \\
		&= \iv{-a_2, a_1} * \iv{b_1, b_2} \\
		&= a_1 b_2 - a_2 b_2
	\end{align*}
	
	Note that the result of the above transformation is equivalent to the determinant of the matrix:
	\begin{equation*}
	\begin{bmatrix}
		a_1 & a_2 \\ b_1 & b_2
	\end{bmatrix}
	\end{equation*}
	
	\subsection{Determinant in $R^3$}
	
	The determinant of matrix $A = \iv{\vec{A}, \vec{B}, \vec{C}}^\top$ is:
	\begin{align*}
		\mo{A} = a_1
		\begin{vmatrix}
			b_2 & b_3 \\ c_2 & c_3
		\end{vmatrix} - a_2
		\begin{vmatrix}
			b_1 & b_3 \\ c_1 & c_3
		\end{vmatrix} + a_3
		\begin{vmatrix} 
			b_1 & b_2 \\ c_1 & c_2
		\end{vmatrix} 
	\end{align*}
	
	The determinant of matrix $\iv{\vec{a}, \vec{b}, \vec{c}}$ is the volume of the parallelepiped formed by edges $\vec{a}$, $\vec{b}$ and $\vec{c}$.
	
	\subsection{Rotation of Vectors}
	
	Vector $\vec{v} = \iv{a, b}$ after $90^\circ$ rotation gives $\iv{-b, a}$. Similarly, a clockwise rotation of $\vec{v}$ in the clockwise direction gives $\iv{b, -a}$.
	
	\subsection{Cross Product's Geometric Interpretation}
	
	The cross product of $\vec{a}$ and $\vec{b}$ is a vector $\vec{a} \times \vec{b}$. The above cross product can be represented via the "determinant" of the matrix:
	\begin{equation*}
	\begin{bmatrix}
		i & j & k \\ a_1 & a_2 & a_3 \\ b_1 & b_2 & b_3
	\end{bmatrix}
	\end{equation*}
	
	The coefficients of $i$, $j$ and $k$ are the $x$, $y$ and $z$ component of the resulting vector respectively.
	
	Consider the parallelepiped $A$ formed by edges $\vec{a}$, $\vec{b}$ and $\vec{c}$. Its volume is the area of the base scaled by the height $h_A$. We obtain:
	\begin{align*}
		\text{Area}(A) &= h_A * \mo{\vec{b} \times \vec{c}} \\
		&= \left(\vec{a} * \frac{\vec{b} \times \vec{c}}{\mo{\vec{b} \times \vec{c}}}\right) * \mo{\vec{b} \times \vec{c}} \\
		&= \vec{a} * (\vec{b} \times \vec{c})
	\end{align*}
	
	The above equation can be expanded via the definition of cross product to display its equality with the determinant of $\iv{\vec{a}, \vec{b}, \vec{c}}^\top$.
	
	\subsubsection{Right Hand Rule}
	
	The right hand rule is stupid. It doesn't work on people who can't tell left from right, like me. Soooooo basically given the cross product $\vec{e} = \vec{a} \times \vec{b}$ where $\vec{e}$ is in the position of poking your eyes out, $\vec{a}$ and $\vec{b}$ are positioned in a counter-clockwise manner.
	
	\section{Matrices}
	
	First, trivially: $$\vec{a} \times \vec{b} = -\vec{b} \times \vec{a}$$
	
	\subsection{Application of Cross Product}
	
	Give $p_1$, $p_2$ and $p_3$ in $\RR^3$ aligned along a hyperplane, the equation for the hyperplane $P(x, y, z)$ denotes the condition for $x$, $y$ and $z$ of a new given point $p$ for it to be contained in the hyperplane.
	
	One approach is to consider if the vectors $\overrightarrow{p_1p_3}$, $\overrightarrow{p_1p_2}$ and $\overrightarrow{p_1p}$ are in the same plane. In order words: $$\det(\iv{\overrightarrow{p_1p}, \overrightarrow{p_1p_2}, \overrightarrow{p_1p_3}}) = 0$$.
	
	Another approach is to obtain the cross product of $\overrightarrow{p_1p_2}$ and $\overrightarrow{p_1p_3}$, and see if the direction of the resultant vector (normal vector) is perpendicular to $\overrightarrow{p_1p}$. In other words: $$\overrightarrow{p_1p} * (\overrightarrow{p_1p_2} \times \overrightarrow{p_1p_3}) =  0$$
	
	\subsection{Linear Relations}
	
	Matrices encapsulate linear transformations. This is useful in scenarios such as change of basis, etc.
	
	Consider a change of basis from $\iv{x_1, x_2, x_3}$ to $\iv{u_1, u_2, u_3}$:
	\begin{align*}
		u_1 &= 2x_1 + 3x_2 + 3x_3 \\
		u_2 &= 2x_1 + 4x_2 + 5x_3 \\
		u_3 &= x_1 + x_2 + 2x_3
	\end{align*}
	
	The above transformation can be represented via a matrix:
	\begin{equation*}
		\begin{bmatrix}
			2 & 3 & 3 \\ 2 & 4 & 5 \\ 1 & 1 & 2
		\end{bmatrix} * \begin{bmatrix}
			x_1 \\ x_2 \\ x_3
		\end{bmatrix} = \begin{bmatrix}
			u_1 \\ u_2 \\ u_3
		\end{bmatrix}
	\end{equation*}
	
	The product of two matrices $A * X$ is the combinations of dot products between the rows of $A$ and the columns of $X$. Therefore, for dimension $A \in \RR^{m*n}$ and $X \in \RR^{n*o}$, $A*X \in \RR^{m*o}$. Trivially, two matrices are only multiplicable if the width of $A$ equals the height of $X$ in $A*X$.
	
	\subsection{Intuition of Matrix Multiplication}
	
	The transformation $AB$ (as matrix product) represents applying transformation $B$ then applying transformation $A$:
	$$(AB)x = A(Bx)$$
	
	In addition, matrix multiplication are not commutative, namely:
	$$AB \neq BA$$
	
	In other words, matrices form a semigroup under multiplication from what we've been currently given (actually a monoid, but identities are covered yet in lecture 3).
	
	(As I'm writing the last sentence the prof literally started to talk about identity matrices\dots Screw it monoid it is.)
	
	An identity matrix $I$ is a matrix that does nothing; it does not impose any linear transformation, and therefore for any matrix $A$:
	\begin{align*}
		AI &= A \\
		IA &= A
	\end{align*}
	
	An identity matrix has $1$s on the diagonal, and $0$s elsewhere. An $\RR^3$ identity matrix:
	\begin{equation*}
		\begin{bmatrix}
			1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1
		\end{bmatrix}
	\end{equation*}
	
	\subsection{Linear Transformation}
	
	For the linear transformation of rotation by $90^\circ$ counter-clockwise, the corresponding matrix is:
	\begin{equation*}
		\begin{bmatrix}
			0 & -1 \\ 1 & 0
		\end{bmatrix}
	\end{equation*}
	
	The above matrix performs the transformation:
	\begin{equation*}
		\begin{bmatrix}
			x \\ y
		\end{bmatrix} \to \begin{bmatrix}
			-y \\ x
		\end{bmatrix}
	\end{equation*}
	
	\subsubsection{Inverting Matrices}
	
	Linear transformations can be inverted (unless in certain scenarios). Similarly, matrices can be inverted, and their inverse represents the inverted linear transformation. Due to this, it is trivial that:
	$$AA^{-1} = I$$
	
	With this property, the equation $AX = B$ (solving for $X$) can be rewritten as $X = A^{-1}B$ as $A^{-1}A = I$ on the LHS.
	
	Generally, the inverse of matrix $A$ is:
	\begin{gather*}
		A^{-1} = \frac{1}{\det(A)} \adj(A)
	\end{gather*}
	
	$\adj(A)$ denotes the adjoint of $A$. To obtain the adjoint matrix we first calculate the minors, which has the same dimension as $A$, and each entry is the determinant of $A$ after removing the row and column that the entry belongs to. Consider:
	\begin{equation*}
		A = \begin{bmatrix}
			2 & 3 & 3 \\ 2 & 4 & 5 \\ 1 & 1 & 2
		\end{bmatrix}
	\end{equation*}
	
	Following the above steps, we obtain:
	\begin{equation*}
		\begin{pmatrix}
			3 & -1 & -2 \\ 3 & 1 & -1 \\ 3 & 4 & 2
		\end{pmatrix}
	\end{equation*}
	
	The cofactors is identical to the minors except for certain inversion in sign (signs are flipped in a checkerboard pattern) ($-$ denotes a flip in sign):
	\begin{equation*}
		\begin{matrix}
			+ & - & + \\ - & + & - \\ + & - & +
		\end{matrix}
	\end{equation*}
	
	Lastly, the matrix is transposed to obtain the adjoint matrix. The adjoint matrix for the above $A$ is:
	\begin{equation*}
		\begin{pmatrix}
			3 & -3 & 3 \\ 1 & 1 & -4 \\ -2 & 1 & 2
		\end{pmatrix}
	\end{equation*}
	
	\section{Equations of Planes}
	
	Recall that an equation for a plane is of the form (where $a$, $b$, $c$ and $d$ are constants):
	\begin{gather*}
		ax + by + cz = d
	\end{gather*}
	
	To illustrate, consider the equation for a plane through the origin with normal vector $\vec{N} = \iv{1, 5, 10}$. Trivially, for any point $\vec{p}$ to be in the plane of interest, $\vec{N} * \vec{p} = 0$.
	
	Similarly, consider the equation for a plane through $P_0 = \iv{2, 1, -1}$ and normal vector $\vec{N} = \iv{1, 5, 10}$, $\overrightarrow{P_0P} * \vec{N} = 0$. In the above case:
	\begin{align*}
		& \overrightarrow{P_0P} * \vec{N} = 0 \\
		\iff & \iv{x - 2, y - 1, z + 1} * \iv{1, 5, 10} = 0 \\
		\iff & (x - 2) + 5(y - 1) + 10(z + 1) = 0 \\
		\iff & x + 5y + 10z = -3
	\end{align*}
	
	An easier method to obtain $-3$ is just to plug $P_0 = \iv{2, 1, -1}$ into $x + 5y + 10z$, as $P_0$ acts as the "origin" of the plane.
	
	To generalize, in equation $ax + by + cz = d$, $\vec{N} = \iv{a, b, c}$ is a normal vector to the plane described by the above equation.
	
	As an example, consider $\vec{v} = \iv{1, 2, -1}$ and the plane described by $x + y + 3z = 5$. One normal vector of the plane is $\vec{N} = \iv{1, 1, 3}$. It is obvious that $\vec{v}$ is not perpendicular to the plane. $\vec{v}$ is perpendicular to $\vec{N}$ since $\vec{N} * \vec{v} = 0$, thus $\vec{v}$ is parallel to the plane.
	
	\subsection{Linear Systems}
	
	A linear system is essentially a bunch of linear equations:
	\begin{align*}
		x + z &= 1 \\
		x + y &= 2 \\
		x + 2y + 3z &= 3
	\end{align*}
	
	To find the common solution to the above system is to find a point $p$ that is contained in all three planes described by the above equations.
	
	The solution is not as trivial when a plane contains the line formed by the other two planes, then there are infinitely many answers, as there are no constraints restricting the line. Similarly, if all three planes are parallel and overlapping, the result is a plane.
	
	There is no solution for a linear system if the planes never intersects.
	
	As a result of the above phenomenon, $AX = B \iff X = A^{-1}B$ might not be always viable depending on the invertibility of the matrix $A$. A matrix $A$ is only invertible if the determinant of it is not $0$, i.e. $\det(A) \neq 0$, as $$\det(A) = 0 \iff \text{one plane is parallel to the intersection line}$$.
	
	\subsection{Homogeneous Case}
	
	A homogeneous case of the linear is when $AX = 0$, i.e. each equation is of the form $ax + by + cz = 0$. An obvious solution is $\iv{0, 0, 0}$, referred to as the \emph{trivial solution}. In other words, all the planes passes through the origin, making the origin a solution to the system.
	
	If $\det{A} \neq 0$, then the inverse of $A$ can be used to solve for $X$. However, this is not very helpful:
	\begin{gather*}
		AX = 0 \iff X = A^{-1}0 \Rightarrow X = \iv{0, 0, 0}
	\end{gather*}
	
	In the case that $\det(A) = 0$, recall that the coefficients of an equation is a normal vector to the plane:
	\begin{align*}
		&\det(A) = 0 \\
		\iff &\det(\vec{N_1}, \vec{N_2}, \vec{N_3}) = 0 \\
		\iff &\text{$\vec{N_1}$, $\vec{N_2}$ and $\vec{N_3}$ are coplanar}
	\end{align*}
	
	Recall that $\det(\vec{N_1}, \vec{N_2}, \vec{N_3}) = 0$ means the volume of the parallelepiped formed by the three vectors is $0$.
	
	Therefore, the solution is the line through $0$ that is perpendicular to $\vec{N_1}$, $\vec{N_2}$ and $\vec{N_3}$ (can be obtained via a simple cross product of the normal vectors). The solution in this case is non-trivial.
	
	\subsection{General Case}
	
	For linear system $AX = B$, if $\det(A) \neq 0$, then a unique solution exists: $X = A^{-1}B$. If $\det(A) = 0$, then either no solutions or infinitely many solutions. We are not yet able to distinguish between the two cases with what the course has currently covered.
	
	
	\section{Equation of Lines}
	
	Deriving from the previous lecture, we can see a line as the intersection of 2 planes. A more convenient representation of a line is by considering it as a trajectory of a moving point. This is referred to as a \emph{parametric equation}.
	
	As an example, consider points $Q_0$ and $Q_1$:
	\begin{align*}
		Q_0 &= \iv{-1, 2, 2} \\
		Q_1 &= \iv{1, 3, -1}
	\end{align*}
	
	The line can be described as a trajectory (originating from $Q_0$) between the two points, namely:
	\begin{align*}
		Q(t) &= Q_0 + t * (Q_1 - Q_0) \\
		&= Q_0 + t * \iv{2, 1, -3}
	\end{align*}
	
	Treating $Q$ as $Q(t) = \iv{x(t), y(t), z(t)}$ and considering point $Q_0$, we obtain that:
	\begin{align*}
		x(t) + 1 &= 2t \\
		y(t) - 2 &= t \\
		z(t ) - 2 &= -3t
	\end{align*}
	
	Reorder and we obtain:
	\begin{align*}
		x(t) &= 2t - 1 \\
		y(t) &= t + 2\\
		z(t ) &= -3t + 2
	\end{align*}
	
	In other words, we've just shown that $Q(t) = Q_0 + t * \overrightarrow{Q_0Q_1}$
	
	\subsection{Applications}
	
	The equation of a line can be used to check if it intersects with a plane and where. Consider the plane $x + 2y + 4z = 7$, what orientation do $Q_0$ and $Q_1$ have in respect to the plane?
	
	By plugging $Q_0$ and $Q_1$ into the plane equation, we obtain:
	\begin{align*}
		x + 2y + 4z &= -1 + 2*2 + 4*2 = 11 > 7 \\
		x + 2y + 4z &= 1 + 2*3 + 4*(-1) = 3 < 7
	\end{align*}
	
	Thus, neither $Q_0$ or $Q_1$ lies in the plane. However, by observing how the result of plugging in the points lie on different subspaces (as denoted by $>7$ and $<7$), $Q_0$ and $Q_1$ are on different side of the plane.
	
	To obtain the intersection of the plane with the line, we plug in $t$ into the line equation:
	\begin{align*}
		&x(t) + 2y(t) + 4z(t) \\
		= &(-1 + 2t) + 2(2 + t) + 4(2 - 3t) \\
		= &-8t + 11
	\end{align*}
	
	This is compared to plane equation: the line intersects at the point where $-8t + 11 = 7$.
	
	The specific point can be obtained via:
	\begin{align*}
		Q(\frac{1}{2}) = \iv{0, \frac{5}{2}, \frac{1}{2}}
	\end{align*}
	
	\subsection{Parametric Equations}
	
	Parametric Equations are excellent at representing a curve/trajectory in space. In general, parametric equations can represent arbitrary motions in a given space.
	
	\begin{defn}
		A \emph{cycloid} is formed as the trajectory of a point on a rolling wheel with radius $r$.
	\end{defn}
	
	This poses the problem of determining the position of the given point, $p \in \RR^2$, after rotation $\theta$:
	\begin{gather*}
		p(\theta) = \iv{x(\theta), y(\theta)}
	\end{gather*}
	
	For a circle centered around $B$ on the horizontal axis (with point $A$ intersecting with the axis on the circumference), the vector $\overrightarrow{OP}$ (where $P$ is another point on the circumference) can be obtained via:
	\begin{gather*}
		\overrightarrow{OP} = \overrightarrow{OA} + \overrightarrow{AB} + \overrightarrow{BP}
	\end{gather*}
	
	Since the wheel is rolling, the arc length on the circumference from $A$ to $P$ is equal to $\mo{\overrightarrow{OA}}$:
	\begin{gather*}
		\overrightarrow{OA} = \iv{a \theta, 0}
	\end{gather*}
	
	$\mo{\overrightarrow{AB}}$ is trivial, as its magnitude is simply the radius of the wheel. With regard to the angle $\theta$, we obtain that:
	\begin{gather*}
		\overrightarrow{BP} = \iv{-a \sin \theta, -a \cos \theta}
	\end{gather*}
	
	Therefore, we obtain that:
	\begin{gather*}
		\overrightarrow{OP} = \iv{a\theta - a \sin \theta, a - a \cos \theta}
 	\end{gather*}
 	
 	\subsection{Taylor Expansion Approximation}
 	
 	In order to explore the shape of the trajectory of $P$ (from the above section) around $\theta = 0$, we can utilize the Taylor expansion to estimate the ratio of $\frac{\Delta y}{\Delta x}$. Consider a wheel with radius $1$:
 	\begin{align*}
 		x(\theta) &= \theta - \sin \theta \\
 		y(\theta) &= 1 - \cos \theta
 	\end{align*}
 	
 	Recall the Taylor approximation:
 	\begin{gather*}
 		\lim_{t \to 0} = f(0) + tf'(0) + \frac{t^2}{2!}f''(0) + \frac{t^3}{3!}f'''(0) + \dots
 	\end{gather*}
 	
 	In our case, we obtain that:
 	\begin{align*}
 		\sin \theta &= \theta - \frac{\theta^3}{6} \\
 		\cos \theta &= 1 - \frac{\theta^2}{2}
 	\end{align*}
 	
 	Plugging back into the original formula, we obtain:
 	\begin{align*}
 		x(\theta) &\approx \theta - (\theta - \frac{\theta^3}{6}) \approx \frac{\theta^3}{6} \\
 		y(\theta) &\approx 1 - (1 - \frac{\theta^2}{2}) \approx \frac{\theta^2}{2}
 	\end{align*}
 	
 	Calculating $y/x$ yields:
 	\begin{gather*}
 		\frac{y}{x} \approx \frac{\theta^2 / 2}{\theta^3 / 6} \approx \frac{3}{\theta}
 	\end{gather*}
 	
 	The above value approaches $\infty$ as $\theta \to 0$.
 	
 	\section{Velocity and Kepler's Second Law}
	
	\begin{defn}
		\emph{Velocity} is the derivative of displacement in respect of time:
		\begin{gather*}
			\vec{v} = \frac{\diff \vec{r}}{\diff t} = \iv{\frac{\diff x}{\diff t}, \frac{\diff y}{\diff t}, \frac{\diff z}{\diff t}}
		\end{gather*}
	\end{defn}
	
	\subsection{Velocity}
	
	In the case of the cycloid (from the previous section), recall that the displacement of point $p$ on the circumference of the wheel ($p = \vec{0}$ initially, when $\theta = 0$) is obtained via:
	\begin{gather*}
		\vec{r}(t) = \iv{t - \sin t, 1 - \cos t}
	\end{gather*}
	
	Its velocity vector is obtained by differentiating the function of each axis:
	\begin{gather*}
		\vec{v} = \iv{1 - \cos t, \sin t}
	\end{gather*}
	
	Notice that when $t=0$, $\vec{v} = \iv{0, 0}$. This is trivial, as $p$ indeed isn't moving at that instant.
	
	To obtain the magnitude of $\vec{v}$, we use Pythagoras theorem:
	\begin{align*}
		\mo{\vec{v}} &= \sqrt{(1 - \cos t)^2 + \sin^2 t} \\
		&= \sqrt{1 - 2 \cos t + \cos^2 t + \sin^2 t} \\
		&= \sqrt{2 - 2 \cos t}
	\end{align*}
	
	\subsection{Acceleration}
	
	\begin{defn}
		\emph{Acceleration} is the derivative of the velocity vector:
		\begin{gather*}
			\vec{a} = \frac{\diff \vec{v}}{\diff \vec{t}}
		\end{gather*}
	\end{defn}
	
	Still considering the above example of cycloid, we notice that the acceleration of $p$ is $\vec{a} = \iv{\sin t, \cos t}$. At point $t = 0$, notice that the acceleration of $p$ is $\vec{0, 1}$, which agrees with the result that we obtained via Taylor approximation.
	
	\subsection{Arc Length}
	\begin{defn}
		The \emph{arc length} is the distance traveled along the curve. In the example above (cycloid), the arc length is simply the portion of the circumference which $p$ has traveled.
	\end{defn}
	
	To relate arc length $s$ to time $t$, notice the relation that the rate of change of arc length is the speed at which the point is moving. Therefore, to obtain the arc length (circumference) of the trajectory of point $p$ with a given function describing $p$'s displacement, we simple integrate the magnitude of the speed (which is equivalent to $\vec{v}$'s magnitude):
	\begin{gather*}
		\int_0^{2\pi}\sqrt{2 - 2 \cos t}\ \diff t
	\end{gather*}
	
	\subsection{Unit Tangent Vector}
	
	\begin{defn}
		A \emph{unit tangent ventor} $\hat{T}$ is the normalized tangent at a given point on the function:
		\begin{gather*}
			\hat{T} = \frac{\vec{v}}{\mo{\vec{v}}}
		\end{gather*}
	\end{defn}
	
	Notice that (where $s$ denotes the speed of $p$):
	\begin{gather*}
		\vec{v} = \frac{\diff \vec{r}}{\diff t} = \frac{\diff \vec{r}}{\diff s}\frac{\diff s}{\diff t}
	\end{gather*}
	
	Therefore, it is trivial that:
	\begin{align*}
		\mo{\vec{v}} &= \frac{\diff s}{\diff t} \\
		\hat{T} &= \frac{\diff s}{\diff t}
	\end{align*}
	
	\subsection{Kepler's Second Law}
	
	Kepler's second law is a good example for exploring the motivation for analyzing motions with vectors. Kepler's laws are about generalizing the motion of planets orbiting a central sun in an ellipse.
	
	\begin{defn}
		\emph{Kepler's second law} states that the planar motion of an orbiting planet satisfies that the rate of the area swept out by the line from the sun to the planet as the planet orbits is constant. 
	\end{defn}
	
	In terms of vectors, consider the position vector $\vec{r}$ originating from the sun to the planet. $\Delta r$ is the planet's change in displacement over $\Delta t$.
	
	Since $\Delta r$ is sufficiently small, we can consider the area of the sector as a triangle enclosed by $\vec{r_1}$, $\vec{r_2}$ and $\Delta \vec{r}$. The area of the sector is then given by (recall that $\Delta \vec{r} = \vec{v} \Delta t$):
	\begin{align*}
		\text{Area} &\approx \frac{1}{2} \mo{\vec{r} \times \Delta \vec{r}} \\
		& \approx \frac{1}{2} \mo{\vec{r} \times \vec{v}} \Delta t
	\end{align*}
	
	Therefore, Kepler's second law is equivalent to stating that $\vec{r} \times \vec{v}$ is constant, and therefore has a derivative of $0$ in respect to $t$:
	\begin{align*}
		&\frac{\diff}{\diff t} (\vec{r} \times \vec{v}) = 0 \\
		\iff &\frac{\diff \vec{r}}{\diff t} \times \vec{v} + \vec{r} \times \frac{\diff v}{\diff t} = 0 \\
		\iff &\vec{v} \times \vec{v} + \vec{r} \times \vec{a} = 0 \\
		\iff &0 + \vec{r} \times \vec{a} = 0 \\
		\iff &\vec{r} \times \vec{a} = 0
	\end{align*}
	
	Therefore, Kepler's second law is stating that the acceleration vector is parallel to the change of position in the orbit.
	
	\section{Multivariable Functions}
	
	A multivariable function is a function that takes in multiple parameters. An example would be $f(x, y) = x^2 + y^2$.
	
	Similar to a regular function, a multivariable function can have a domain:
	\begin{align*}
		f(x, y) &= \sqrt{y},\ y \geq 0 \\
		f(x, y) &= \frac{1}{x + y},\ x + y \neq 0
	\end{align*}
	
	The visualization of such functions can be a bit tricky. Consider the case of a 2-variable function, its graph can be plotted in an $\RR^3$ space where the height at $(x, y)$ is $f(x, y)$.
	
	Consider the function $f(x, y) = 1 - x^2 - y^2$. To plot a graph for this function, consider the graph on the individual xz-plane and yz-plane (i.e. when the other axis is at $0$). This will result in $z = 1 - x^2$ and $z = 1 - y^2$ respectively.
	
	Note that the shape of the intersection of function $f(x, y) = 1 - x^2 - y^2$ on the xy-plane can be obtained via setting the value on the z-axis to $0$, i.e.:
	\begin{align*}
		&1 - x^2 - y^2 = 0 \\
		\iff &x^2 + y^2 = 0
	\end{align*}
	
	Thus, the intersection is a unit circle.
	
	\subsection{Contour Graph}
	
	A contour graph is a slice of the graph of a 2-variable function by a horizontal plane. Each section in a contour map represents ab area of (approximately) same height.
	
	Consider the contour plot for function $f(x, y) = 1 - x^2 - x^2$, its contour plot consists of various circles around the origin. The density of circles increases the further away from the origin, indicating an increase in the slope of the function (as less distance for a change of $1$ in height).
	
	\subsection{Derivatives}
	
	In a regular function $f(x)$, the derivative of $f(x)$ is its change in respect to $x$, namely:
	\begin{equation*}
		\lim_{h \to 0} \frac{f(x + h) - f(x)}{h}
	\end{equation*}
	
	In other words, the derivative of a function is a function of its slope.
	
	\subsection{Linear Approximation Formula}
	
	To approximate $f(x)$ when given $f(x_0)$ where $x_0$ is a value close to $x$, the value $f(x)$ can be approximated via $f(x)$'s first derivative:
	\begin{equation*}
		f(x) = f(x_0) + f'(x_0) * (x - x_0)
	\end{equation*}
	
	\subsection{Partial Derivatives}
	
	With a function with more than one parameter, a derivative can be taken with respect to any one parameter. This can be interpreted as the change in the output value in respect to a change in one of its parameter.
	
	For example, consider a 2-variable function $f(x, y)$. Its partial derivative in respect to $x$ is defined as:
	\begin{gather*}
		\frac{\partial f}{\partial x}(x, y) = \lim_{h \to 0} \frac{f(x + h, y) - f(x, y)}{h}
	\end{gather*}
	
	This can be interpreted similar to the "slice by a plane" analogy used previously when describing contour graphs. To differentiate in respect to $x$ is to consider the change in the hyperplane of all other axis when $x$ changes (e.g. the change in the yz-plane when $x$ moves in $f(x, y)$).
	
	\section{Partial Derivatives and Tangents}
	
	As mentioned previously, a 2-variable function $f(x, y)$ has two derivatives each in the direction of an axis.
	
	Consider its tangent line $L_x$; if $\frac{\partial f}{\partial x}(x_0, y_0) = a$, then the line is described by $y = y_0$ and $z = z_0 + a(x - x_0)$.
	
	Similarly, the tangent line $L_y$ being described by $\frac{\partial f}{\partial y}(x_0, y_0) = b$ is the line specified by $x = x_0$ and $z = z_0 + b(y - y_0)$.
	
	$L_x$ and $L_y$ are both tangent to the graph $f(x, y)$, and determines the plane
	\begin{gather*}
		z = z_0 + a(x - x_0) + b(y - y_0)
	\end{gather*}
	
	Note that by fixing either $x$ or $y$ in the above equation, the tangent for $y$ and $x$ is obtained respectively. The above plane is the tangent plane of $f$ at a given point $(x, y)$.
	
	\subsection{Minima and Maxima}
	
	If a given point $(x, y)$ is the local minimum or maximum, then its derivative in either direction is $0$. This is equivalent to stating that the tangent plane at a minimum/maximum is a horizontal plane.
	
	Note that having $0$ as all partial derivatives is a necessary but insufficient condition for a minimum/maximum.
	
	Such a point $(x_0, y_0)$ is referred to as a \emph{critical point} of $f$, given that $f_x(x_0, y_0) = 0$ and $f_y(x_0, y_0) = 0$ (note the partial derivative notation of $f$).
	
	Consider the function $f(x, y) = x^2 - 2xy + 3y^2 + 2x - 2y$. To identify the critical points of $f$, its partial derivatives can be taken:
	\begin{align*}
		\frac{\partial f}{\partial x}(x, y) &= 2x - 2y + 2 \\
		\frac{\partial f}{\partial y}(x, y) &= -2x + 6y - 2
	\end{align*}
	
	The critical point can thus be obtained by solving the system:
	\begin{align*}
		2x - 2y + 2 &= 0 \\
		-2x + 6y - 2 &= 0
	\end{align*}
	
	Thus, the only critical point of $f$ is $(-1, 0)$; however, identifying the critical point is insufficient to identify whether it is a maximum or minimum; a critical point can be a minimum (positive second derivatives in all axes), a maximum (negative second derivatives in all axes), or a saddle point (differently signed second derivative in different directions, i.e. anything that is not an extremum).
	
	Note that the above $f$ can be rewritten by completing the square:
	\begin{align*}
		f(x, y) &= (x - y)^2 + 2y^2 + 2x - 2y \\
		&= ((x - y) + 1)^2 + 2y^2 - 1
	\end{align*}
	
	In this form, it is trivial to identify the minimum of $f$. Since both $((x - y) + 1)^2$ and $y^2$ are squares, their minimum value is $0$, thus the minimum of $f$ is $0 - 1 = -1$. However such an approach is obviously not feasible for all functions.
	
	\subsection{Application of Minimum/Maximum}
	
	Least-square interpolation is to find the best fit line given a set of data. The following example considers a 2D dataset of points $(x, y)$ and fits the line $y = mx + b$.
	
	The loss function can be interpreted as how bad a line fits the data, and the aim of least-square interpolation is to minimize the loss function. This is simply a sum of the square of deviations for each point in the dataset:
	\begin{gather*}
		\min_{m, b} \sum^n_{i=1} [y_i - (mx_i + b)]^2
	\end{gather*}
	
	To minimize such a function, we can identify its critical point by taking the function's partial derivatives in respect to $m$ and $b$ (where $L$ is the loss function):
	\begin{align*}
		\frac{\partial L}{\partial m} &= \sum^n_{i=1} [2 * (y_i - (mx_i + b)) * (-x_i)] \\
		\frac{\partial L}{\partial b} &= \sum^n_{i=1} [2 * (y_i - (mx_i + b)) * (-1)]
	\end{align*}
	
	To obtain the critical point of the loss function, we find $m$ and $b$ such that both derivatives is zero. With some reordering:
	\begin{align*}
		\sum^n_{i=1} \left(mx^2_i + bx_i - x_iy_i \right) = 0 &\iff \left(\sum^n_{i=1}x^2_i \right)m + \left(\sum^n_{i=1}x_i \right) b = \sum^n_{i=1}x_iy_i\\
		\sum^n_{i=1} (mx_i + b - y_i) = 0 &\iff \left(\sum^n_{i=1} x_i\right)m + nb = \sum^n_{i=1} y_i
	\end{align*}
	
	The above is simply a linear system.
	
	\section{Global Maximum/Minimum}
	
	Recall that a critical point is a point where the tangent plane is horizontal. This could be a local minimum, local maximum, or saddle point.
	
	To obtain the global min/max, however, requires more information about the function, such as how it behaves at $\infty$ (or its boundary). Note that the global min/max can only occur at either a critical point or on the function's boundary (or $\infty$ for an unbounded function).
	
	Similar to a single variable function, the second derivative can be used to gain more observations of the behaviors of the function.
	
	\subsection{Second Derivative Test}
	
	Consider the function $w = a^2 + bxy + cy^2$. This function has a critical point at the origin. With a linear approximation by plugging in a sufficiently small $x$ or $y$ value, it can be observed that the tangent plane at the origin is indeed horizontal.
	
	Recall the example from last lecture $w = x^2 + 2xy + 3y^2$. This can be rewritten as $w = (x + y)^2 + 2y^2$. Being the sum of two squares, the origin would be a minimum as both terms are non-negative.
	
	Similarly, such a method can be applied to the generalized $w = a^2 + bxy + cy^2$ (where $a \neq 0$):
	\begin{align*}
		w &= a(x^2 + \frac{b}{a}xy) + cy^2 \\
		&= a(x + \frac{b}{2a}y)^2 + (c - \frac{b^2}{4a})y^2 \\
		&= \frac{1}{4a}[4a^2(x+\frac{b}{2a}y)^2 + (4ac-b^2)y^2]
	\end{align*}
	
	This is yet another a sum of two squares, and therefore it can be observed that going in any direction from the origin results in an increase in height, thus concluding that the origin is the minimum.
	
	\subsection{Cases of Quadratics}
	
	Considering the above form of sum of two squares for quadratics, there are three cases of quadratic functions:
	\begin{enumerate}
		\item $4ac - b^2 < 0$: The first term would be $\geq 0$, while the second term would be $\leq 0$. Therefore, the critical point of this quadratic is a saddle point.
		\item $4ac - b^2 = 0$: The second term would be $0$, thus the height along a certain direction would be $0$. In other words, the entire axis would be critical points. This is referred to as the degenerate critical point.
		\item $4ac - b^2 > 0$: The equation would simply be the sum of two squares, and therefore the behavior of the function is dependent on the sign of $a$ as it dictates the sign of the function. If $a > 0$, the function would be increasing in all directions from the origin, thereby making it a minimum. Dually, if $a < 0$, the origin would be a maximum.
	\end{enumerate}
	
	Note the resemblance of $4ac - b^2$ to the discriminant $\Delta = b^2 - 4ac$. Recall the $w$ function $w = ax^2 + bxy + cy^2$. It can be reordered as:
	\begin{gather*}
		w = y^2 [a(\frac{x}{y})^2 + b(\frac{x}{y})+ c]
	\end{gather*}
	
	If $\Delta > 0$, then there exists multiple solutions, thereby the result of $w$ spans both the $> 0$ space and the $< 0$ space (as the function crosses w = 0). Therefore, the critical point of $w$ is a saddle point.
	
	If $\Delta < 0$, then there is no intersection of the function with the $w = 0$ plane. In this case, the function is either always positive or always negative depending on the sign of $a$. Therefore, the critical point is either a maximum or a minimum.
	
	\subsection{Generalized Case}
	
	For a generic function, a similar analysis method can be incorporated by observing its second derivatives.
	
	For simplicity, another symbol for the second derivative of a function $f$ is:
	\begin{gather*}
		f_{xy} = \frac{\partial^2 f}{\partial x \partial y}
	\end{gather*}
	
	Note that the order of taking the derivatives does not matter. In other words, $f_{xy} = f_{yx}$.
	
	Thus, we can conclude that for a two-variable function, there are three second derivatives: $f_{xx}$, $f_{xy}$ (= $f_{yx}$), and $f_{yy}$.
	
	\begin{defn}
		The \emph{second derivative test} states that at a critical point $p = (x_0, y_0)$ of function $f$, then there are four cases to consider:
		\begin{enumerate}
			\item $f_{xx}(p) * f_{yy}(p) - f_{xy}(p)^2 > 0$ and $f_{xx}(p) > 0$: The critical point is a local minimum.
			\item $f_{xx}(p) * f_{yy}(p) - f_{xy}(p)^2 > 0$ and $f_{xx}(p) < 0$: The critical point is a local maximum.
			\item $f_{xx}(p) * f_{yy}(p) - f_{xy}(p)^2 < 0$: The critical point is a saddle point.
			\item $f_{xx}(p) * f_{yy}(p) - f_{xy}(p)^2 = 0$: Cannot conclude.
		\end{enumerate}
	\end{defn}
	
	To verify, consider the special case $w = ax^2 + bxy + cy^2$ mentioned above. Its first order derivatives are:
	\begin{align*}
		w_x &= 2ax + by \\
		w_y &= bx + 2cy
	\end{align*}
	
	By further differentiation, we obtain:
	\begin{align*}
		w_{xx} &= 2a \\
		w_{xy} &= b\\
		w_{yy} &= 2c
	\end{align*}
	
	Thus, $f_{xx}(p) * f_{yy}(p) - f_{xy}(p)^2$ is $4ac - b^2$ in this case. By considering the different cases above, it can be observed that this special case indeed satisfies the rules described above.
	
	\subsection{Quadratic Approximation}
	
	Consider the function $f(x, y)$, the change in $f$ as $x$ or $y$ changes a little bit should be able to be approximated by:
	\begin{gather*}
		\Delta f \approx f_x * (x - x_0) + f_y * (y - y_0)
	\end{gather*}
	
	However, note that $\Delta f = 0$ at critical points. To better approximate the function, its second derivative can be considered:
	\begin{align*}
		f &\approx f_x * (x - x_0) + f_y * (y - y_0) \\
		&+ \frac{1}{2} f_{xx} * (x - x_0)^2 + f_{xy} (x - x_0)(y - y_0) \\
		&+ \frac{1}{2} f_{yy} * (y - y_0)^2
	\end{align*}
	
	To acquire a more accurate approximation, the function's higher order derivatives can be considered. However, the current second order approximation reduces the generalized $f$ to a quadratic function. Note that the coefficients of the quadratic terms correspond to the $a$, $b$ and $c$ terms in a quadratic as mentioned before, yet scaled by $\frac{1}{2}$.
	
	In the degenerate case, however, the above approximation formula is only reasonable only if the higher order terms are negligible. Since in a non-degenerate case, the shape of the formula is determined by the quadratic terms, yet in the degenerate case, a small shift in the critical point axis can alter the shape of the function such that the critical point is a minimum/maximum, thus not qualifying as the degenerate case. In other words, what happens in the degenerate case depends on higher order derivatives (this will not be covered in this class, so we refer to it as "cannot conclude" at the moment).
	
	For example, consider the function where $x, y > 0$:
	\begin{gather*}
		f(x, y) = x + y + \frac{1}{xy}
	\end{gather*}
	
	To obtain the critical points for the above function, we compute the first derivative:
	\begin{align*}
		f_x &= 1 - \frac{1}{x^2y} \\
		f_y &= 1 - \frac{1}{xy^2}
	\end{align*}
	
	The equation for the critical point(s) become:
	\begin{align*}
		x^2y = 1 \\
		xy^2 = 1
	\end{align*}
	
	Thus $x=1$ and $y=1$, at which point the second derivatives of $f$ is:
	\begin{align*}
		f_{xx} &= \frac{2}{x^3y} \\
		f_{xy} &= \frac{1}{x^2y^2} \\
		f_{yy} &= \frac{2}{xy^3}
	\end{align*}
	
	By computing $AC - B^2$, we obtain that $(1, 1)$ is either a local minimum or maximum. Since $A > 0$ (or $f_{xx} > 0$), the value of the function as $x$ approaches $\infty$ is $\infty$, thus $f(1, 1)$ is a local minimum.
	
	
	
\end{document}
